{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivgold/Neural-Network-From-Scratch/blob/main/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5PtfiI3TFpx"
      },
      "source": [
        "# Imports cell\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2zIjMgcmib5"
      },
      "source": [
        "# Section 1 - Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH6tRDQqkCDD"
      },
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "  \"\"\"\n",
        "  layer_dims: ndarray of shape(num_of_layers, )\n",
        "  \"\"\"\n",
        "  layer_weights = {}\n",
        "  for layer_index in range(1, len(layer_dims)):\n",
        "    prev_layer_dim = layer_dims[layer_index-1]\n",
        "    current_layer_dim = layer_dims[layer_index]\n",
        "\n",
        "    # initialize weight and bias values\n",
        "    layer_weights[f\"W{layer_index}\"] = np.random.randn(current_layer_dim, prev_layer_dim) * np.sqrt(2/prev_layer_dim)\n",
        "    layer_weights[f\"B{layer_index}\"] = np.zeros((current_layer_dim, 1))\n",
        "\n",
        "  return layer_weights\n",
        "\n",
        "\n",
        "def linear_forward(A, W, B):\n",
        "  \"\"\"\n",
        "  A: ndarray of shape (size_of_prev_layer, num_of_examples)\n",
        "  W: ndarray of shape (size_of_current_layer, size_of_prev_layer)\n",
        "  B: ndarray of shape (size_of_current_layer, )\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate Z values for current layer\n",
        "  Z = W @ A + B\n",
        "  linear_cache = {\n",
        "      'A': A,\n",
        "      'W': W,\n",
        "      'B': B\n",
        "  }\n",
        "  return Z, linear_cache\n",
        "\n",
        "\n",
        "def softmax(Z):\n",
        "  \"\"\"\n",
        "  Z: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  \"\"\"\n",
        "  exp_max = np.exp(Z - np.max(Z,axis=0,keepdims=True))\n",
        "  A = exp_max/np.sum(exp_max,axis=0,keepdims=True) \n",
        "\n",
        "  return A, Z\n",
        "\n",
        "\n",
        "def relu(Z):\n",
        "  \"\"\"\n",
        "  Z: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  \"\"\"\n",
        "  A = np.where(Z > 0, Z, 0)\n",
        "\n",
        "  return A, Z\n",
        "\n",
        "\n",
        "def linear_activation_forward(A_prev, W, B, activation):\n",
        "  \"\"\"\n",
        "  A_prev: ndarray of shape (size_of_prev_layer, num_of_examples)\n",
        "  W: ndarray of shape (size_of_prev_layer, size_of_current_layer)\n",
        "  B: ndarray of shape (size_of_current_layer, )\n",
        "  activation: an implemented python's activation function\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate Z and activation values for current layer\n",
        "  Z, linear_cache = linear_forward(A_prev, W, B)\n",
        "  A, activation_cache = activation(Z)\n",
        "  \n",
        "  linear_cache.update({\"Z\": activation_cache})\n",
        "  return A, linear_cache\n",
        "\n",
        "\n",
        "def L_model_forward(X, parameters, use_batchnorm):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (input_size, num_of_examples)\n",
        "  parameters: python's dictionary containing the initilized weights of the network\n",
        "  use_batchnorm: a boolean flag used to determine whether to apply batchnorm after activation\n",
        "  \"\"\"\n",
        "  num_of_layers = int(len(parameters.keys())/2)\n",
        "\n",
        "  A_prev = X\n",
        "  W = None\n",
        "  B = None\n",
        "  caches = []\n",
        "  # the forward-propagation until the output layer\n",
        "  for layer_index in range(1, num_of_layers):\n",
        "    W = parameters[f\"W{layer_index}\"]\n",
        "    B = parameters[f\"B{layer_index}\"]\n",
        "    A_new, cache = linear_activation_forward(A_prev, W, B, relu)\n",
        "    if use_batchnorm:\n",
        "      A_new = apply_batchnorm(A_new)\n",
        "\n",
        "    # add cache to caches and update A_prev\n",
        "    caches.append(cache)\n",
        "    A_prev = A_new\n",
        "  \n",
        "  # forward the output layer\n",
        "  W_output = parameters[f\"W{num_of_layers}\"]\n",
        "  B_output = parameters[f\"B{num_of_layers}\"]\n",
        "  AL, cache_output = linear_activation_forward(A_prev, W_output, B_output, softmax)\n",
        "  \n",
        "  caches.append(cache_output)\n",
        "  return AL, caches\n",
        "\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "  \"\"\"\n",
        "  AL: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  \"\"\"\n",
        "  epsilon = 1e-12\n",
        "  # Calculate cost based on the loss function:\n",
        "  loss = np.sum(np.multiply(Y, np.log(AL+epsilon)), axis=0)\n",
        "  cost = -np.mean(loss)\n",
        "  return cost\n",
        "\n",
        "\n",
        "def apply_batchnorm(A):\n",
        "  \"\"\"\n",
        "  A: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  \"\"\"\n",
        "  epsilon = 7e-1\n",
        "  # Calculating maen and var of currnet layer-batch activations values \n",
        "  mu = np.mean(A, axis=1, keepdims=True)\n",
        "  var = np.var(A, axis=1, keepdims=True)\n",
        "  \n",
        "  # Normalizing activations values \n",
        "  NA = (A - mu) / (np.sqrt(var + epsilon))\n",
        "  return NA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJG1xEMImmEy"
      },
      "source": [
        "# Section 2 - Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJJk1DcruqCB"
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "  \"\"\"\n",
        "  dZ: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  cache: tuple of values (A_prev, W, B) coming from the forward propagation in the current layer\n",
        "  \"\"\"\n",
        "  # A_prev: (size_of_prev_layer, num_of_examples), W: (size_of_current_layer, size_of_prev_layer), B: (size_of_current_layer)\n",
        "  A_prev, W, B = cache\n",
        "  num_of_examples = A_prev.shape[1]\n",
        "\n",
        "  # Z = A_prev @ W + b, calculating grads for W, A_prev, B of current layer\n",
        "  dA_prev = W.T @ dZ\n",
        "  dW = (dZ @ A_prev.T) / num_of_examples\n",
        "  dB = np.mean(dZ, axis=1, keepdims=True)\n",
        "  return dA_prev, dW, dB\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "  \"\"\"\n",
        "  dA: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  cache: contains both the linear cache (dict of A_prev, W, B) and the activations cache (Z)\n",
        "  activation: the activation function to be used (str, either \"softmax\" or \"relu\")\n",
        "  \"\"\"\n",
        "  linear_cache = (cache['A'], cache['W'], cache['B']) \n",
        "  # Separating 2 cased activations: Relu, Softmax\n",
        "  if activation == 'relu':\n",
        "    dZ = relu_backward(dA, cache['Z'])\n",
        "  elif activation == 'softmax':\n",
        "    dZ = softmax_backward(dA, cache['AL'])\n",
        "\n",
        "  return linear_backward(dZ, linear_cache)\n",
        "  \n",
        "def relu_backward(dA, activation_cache):\n",
        "  \"\"\"\n",
        "  dA: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  activation_cache: containing the calcualted Z value from the forward propagation\n",
        "  \"\"\"\n",
        "  # Calculating current layer dZ values based on dA (relu)\n",
        "  Z = activation_cache\n",
        "  dZ = np.where(Z > 0, dA, 0)\n",
        "  return dZ\n",
        "\n",
        "def softmax_backward(dA, activation_cache):\n",
        "  \"\"\"\n",
        "  dA: Y\n",
        "  activation_cache: containing the calcualted Z value from the forward propagation\n",
        "  \"\"\"\n",
        "  # Calculating current layer dZ values based on dA (softmax)\n",
        "  AL = activation_cache\n",
        "  Y = dA\n",
        "  dZ = AL - Y\n",
        "  return dZ\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "  \"\"\"\n",
        "  AL: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  caches: all of the caches from the forward propagation\n",
        "  \"\"\"\n",
        "  grads = {}\n",
        "  num_of_layers = len(caches)\n",
        "\n",
        "  # calculating the gradients of the output layer\n",
        "  caches[-1].update({\"AL\": AL})\n",
        "  dA, dW, dB = linear_activation_backward(Y, caches[-1], \"softmax\")\n",
        "  grads[f\"dA{num_of_layers}\"] = dA\n",
        "  grads[f\"dW{num_of_layers}\"] = dW\n",
        "  grads[f\"dB{num_of_layers}\"] = dB\n",
        "\n",
        "  # calculating the gradients of all of the other hidden layers\n",
        "  for cache_index in reversed(range(num_of_layers-1)):\n",
        "    dA, dW, dB = linear_activation_backward(dA, caches[cache_index], \"relu\")\n",
        "    grads[f\"dA{cache_index+1}\"] = dA\n",
        "    grads[f\"dW{cache_index+1}\"] = dW\n",
        "    grads[f\"dB{cache_index+1}\"] = dB\n",
        "  \n",
        "  return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  \"\"\"\n",
        "  parameters: python's dictionary containing the initilized weights of the network\n",
        "  grads: python's dictionary containnig the gradients of all of the parameters\n",
        "  \"\"\"\n",
        "  updated_parameters = {}\n",
        "  # Update weights andbias values:\n",
        "  for parameter in parameters.keys():\n",
        "    updated_parameters[parameter] = parameters[parameter] - learning_rate * grads[f\"d{parameter}\"]\n",
        "  \n",
        "  return updated_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paAYzucyUqv6"
      },
      "source": [
        "# Section 3 - Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4-ADqNB8Sd"
      },
      "source": [
        "def L_layer_model(X, Y, layer_dims, learning_rate, num_iterations, batch_size):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (height*weight, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  layer_dims: ndarray of shape (num_of_layers)\n",
        "  batch_size: the number of examples in a single training batch\n",
        "  \"\"\"\n",
        "\n",
        "  def train_val_split(X, Y, train_size=0.8):\n",
        "    \"\"\"\n",
        "    The function split randomly the training data into train and validation set with a ratio given by train_size parameter\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: ndarray of shape (height*weight, num_of_examples)\n",
        "    Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "    train_size: float between 0-1, represent the training size\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x_train: The actual training set data, numpy array of shape (input_size, num_training_examples)\n",
        "    y_train: The actual training labels, numpy array of shape (num_of_classes, num_training_examples)\n",
        "    x_validation: The validation set data, numpy array of shape (input_size, num_validation_examples)\n",
        "    y_validation: The validation labels, numpy array of shape (num_of_classes, num_validation_examples)\n",
        "    \"\"\"\n",
        "    num_of_examples = X.shape[1]\n",
        "    indices = np.random.permutation(num_of_examples)\n",
        "    num_of_train_examples = int(num_of_examples*train_size)\n",
        "    # Taking indexes of train and validation\n",
        "    training_idx, validation_idx = indices[:num_of_train_examples], indices[num_of_train_examples:]\n",
        "    # Splitting the data:\n",
        "    x_train, y_train, x_validation, y_validation = X[:, training_idx], Y[:, training_idx], X[:, validation_idx], Y[:, validation_idx]\n",
        "    return x_train, y_train, x_validation, y_validation\n",
        "  \n",
        "  # Splitting the data:\n",
        "  x_train, y_train, x_validation, y_validation = train_val_split(X, Y)\n",
        "\n",
        "  print(\"training examples: \", x_train.shape[1])\n",
        "  print(\"validation examples: \", x_validation.shape[1])\n",
        "  \n",
        "  # initialize\n",
        "  parameters = initialize_parameters(layer_dims)\n",
        "  costs = []\n",
        "  \n",
        "  # report variables\n",
        "  training_iteration = 0\n",
        "  epochs = 0\n",
        "\n",
        "  # stopping criterion variables\n",
        "  ITERATION_TO_IMPROVE = 100\n",
        "  MAX_SMALL_IMPROVEMENT = 0.001\n",
        "  stop = False\n",
        "  last_cost = np.inf\n",
        "\n",
        "  # storing the best weights achieved so far\n",
        "  last_parameters = parameters\n",
        "\n",
        "  while not stop:\n",
        "    epochs += 1\n",
        "\n",
        "    # split the data to batches\n",
        "    for batch_data, batch_targets in list(zip(np.array_split(x_train, x_train.shape[1]/batch_size, axis=1), np.array_split(y_train, y_train.shape[1]/batch_size, axis=1))):\n",
        "      training_iteration+=1\n",
        "      # Update params by batch values:\n",
        "      AL, caches = L_model_forward(batch_data, parameters, False)\n",
        "      grads = L_model_backward(AL, batch_targets, caches)\n",
        "      parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "      # computing validation set cost\n",
        "      validation_AL, validation_caches = L_model_forward(x_validation, parameters, False)\n",
        "      validation_set_cost = compute_cost(validation_AL, y_validation)\n",
        "\n",
        "      if training_iteration % 100 == 0:\n",
        "        # save cost\n",
        "        costs.append(validation_set_cost)\n",
        "        print_progress = int((training_iteration/num_iterations)*100)\n",
        "        print(\"=\"+\"=\"*print_progress+\">\"+\".\"*(100-print_progress))\n",
        "        print(f\"training iteration: {training_iteration}/{num_iterations}\")\n",
        "        print(\"validation cost: \", validation_set_cost)\n",
        "        print(\"-\"*102)\n",
        "        \n",
        "      if validation_set_cost + MAX_SMALL_IMPROVEMENT >= last_cost:\n",
        "        ITERATION_TO_IMPROVE -= 1\n",
        "      else:\n",
        "        # update last cost, save best weights so far, and reset the iteration to improve\n",
        "        last_cost = validation_set_cost\n",
        "        last_parameters = parameters\n",
        "        ITERATION_TO_IMPROVE = 100\n",
        "      \n",
        "      if ITERATION_TO_IMPROVE == 0:\n",
        "        # stop training if had `ITERATION_TO_IMPROVE` iteration without or with `MAX_SMAL_IMPROVEMENT` improvement\n",
        "        stop=True\n",
        "        break\n",
        "    \n",
        "      if training_iteration == num_iterations:\n",
        "        stop=True\n",
        "        break\n",
        "\n",
        "  print(f\"Training done after {epochs} epochs and {training_iteration}/{num_iterations} iterations\")\n",
        "  final_train_acc = predict(x_train, y_train, last_parameters)\n",
        "  final_validation_acc = predict(x_validation, y_validation, last_parameters)\n",
        "  print(\"Final Train Accuracy: {:.3f}\".format(final_train_acc*100))\n",
        "  print(\"Final Validation Accuracy: {:.3f}\".format(final_validation_acc*100))\n",
        "  return last_parameters, costs\n",
        "\n",
        "\n",
        "def predict(X, Y, parameters):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (height*weight, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  parameters: network learned weights\n",
        "  \"\"\"\n",
        "  # shape: (num_of_classes, num_of_examples)\n",
        "  outputs, caches = L_model_forward(X, parameters, False)\n",
        "  # shape: (num_of_classes, num_of_examples)\n",
        "  preds = np.zeros_like(outputs)\n",
        "  # shape: (num_of_examples)\n",
        "  network_argmax = np.argmax(outputs, axis=0)\n",
        "  preds[network_argmax, np.arange(preds.shape[1])] = 1\n",
        "\n",
        "  # calculate accuracy\n",
        "  correct_class = 0\n",
        "  num_of_examples = Y.shape[1]\n",
        "  for sample_index in range(num_of_examples):\n",
        "    sample_pred = preds[:, sample_index]\n",
        "    sample_target = Y[:, sample_index]\n",
        "    if np.array_equal(sample_pred, sample_target):\n",
        "      correct_class += 1\n",
        "  \n",
        "  accuracy = correct_class / num_of_examples\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbH2CdZTmrEs"
      },
      "source": [
        "# Section 4 - Regular Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tul4mGgg0gnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95d3e7b-964b-47ab-e8fb-77ef46a211c0"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
        "\n",
        "# flatten the image\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# normalize the input\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "\n",
        "def encode_labels(y):\n",
        "  \"\"\"\n",
        "  Performs one-hot encoding to a label vector.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  y: ndarray of shape(num_of_examples, )\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  y_encoded: The one-hot encoding of the given label vector y.\n",
        "  \"\"\"\n",
        "  y_encoded = np.zeros((y.size, y.max()+1), dtype=int)\n",
        "  y_encoded[np.arange(y.size), y] = 1\n",
        "  return y_encoded\n",
        "\n",
        "# encode the mnsit labels\n",
        "y_train, y_test = encode_labels(y_train), encode_labels(y_test)\n",
        "# define layer dims\n",
        "layer_dims = np.array([x_train.shape[1], 20, 7, 5, 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-KHWOUNPv53",
        "outputId": "afec9316-05ce-4786-a80e-99135c3f4b4b"
      },
      "source": [
        "np.random.seed(42)\n",
        "print(\"Start Training\")\n",
        "trained_params, costs = L_layer_model(x_train.T, y_train.T, layer_dims, 0.009, 10000, 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples:  48000\n",
            "validation examples:  12000\n",
            "==>...................................................................................................\n",
            "training iteration: 100/10000\n",
            "validation cost:  2.130688830167577\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===>..................................................................................................\n",
            "training iteration: 200/10000\n",
            "validation cost:  1.9920518212247604\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====>.................................................................................................\n",
            "training iteration: 300/10000\n",
            "validation cost:  1.8754808790988269\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====>................................................................................................\n",
            "training iteration: 400/10000\n",
            "validation cost:  1.7670501353897101\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======>...............................................................................................\n",
            "training iteration: 500/10000\n",
            "validation cost:  1.6622083482183418\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======>..............................................................................................\n",
            "training iteration: 600/10000\n",
            "validation cost:  1.5673767611503393\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========>.............................................................................................\n",
            "training iteration: 700/10000\n",
            "validation cost:  1.4822078608646727\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========>............................................................................................\n",
            "training iteration: 800/10000\n",
            "validation cost:  1.3859430936831434\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========>...........................................................................................\n",
            "training iteration: 900/10000\n",
            "validation cost:  1.2962556554136722\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========>..........................................................................................\n",
            "training iteration: 1000/10000\n",
            "validation cost:  1.2106899787009149\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============>.........................................................................................\n",
            "training iteration: 1100/10000\n",
            "validation cost:  1.1363926373549207\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============>........................................................................................\n",
            "training iteration: 1200/10000\n",
            "validation cost:  1.0634220114703714\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============>.......................................................................................\n",
            "training iteration: 1300/10000\n",
            "validation cost:  1.0125273679530358\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============>......................................................................................\n",
            "training iteration: 1400/10000\n",
            "validation cost:  0.964058606529735\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================>.....................................................................................\n",
            "training iteration: 1500/10000\n",
            "validation cost:  0.9081077850312061\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================>....................................................................................\n",
            "training iteration: 1600/10000\n",
            "validation cost:  0.8684655898241856\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================>...................................................................................\n",
            "training iteration: 1700/10000\n",
            "validation cost:  0.8360529494016303\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================>..................................................................................\n",
            "training iteration: 1800/10000\n",
            "validation cost:  0.8025928315924619\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================>.................................................................................\n",
            "training iteration: 1900/10000\n",
            "validation cost:  0.7768831996285376\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================>................................................................................\n",
            "training iteration: 2000/10000\n",
            "validation cost:  0.753196741803441\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================>...............................................................................\n",
            "training iteration: 2100/10000\n",
            "validation cost:  0.7183891940458722\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================>..............................................................................\n",
            "training iteration: 2200/10000\n",
            "validation cost:  0.6928764623550218\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================>.............................................................................\n",
            "training iteration: 2300/10000\n",
            "validation cost:  0.6615482827864821\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================>............................................................................\n",
            "training iteration: 2400/10000\n",
            "validation cost:  0.639115782967608\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================>...........................................................................\n",
            "training iteration: 2500/10000\n",
            "validation cost:  0.6075503915183342\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================>..........................................................................\n",
            "training iteration: 2600/10000\n",
            "validation cost:  0.5890539767889504\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================>.........................................................................\n",
            "training iteration: 2700/10000\n",
            "validation cost:  0.5678954264069318\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================>........................................................................\n",
            "training iteration: 2800/10000\n",
            "validation cost:  0.5539117886665311\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================>........................................................................\n",
            "training iteration: 2900/10000\n",
            "validation cost:  0.5308380133439923\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================>......................................................................\n",
            "training iteration: 3000/10000\n",
            "validation cost:  0.5128033201283827\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================>.....................................................................\n",
            "training iteration: 3100/10000\n",
            "validation cost:  0.49563184416985717\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================>....................................................................\n",
            "training iteration: 3200/10000\n",
            "validation cost:  0.48810454092729205\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================>...................................................................\n",
            "training iteration: 3300/10000\n",
            "validation cost:  0.4739142334745376\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================================>..................................................................\n",
            "training iteration: 3400/10000\n",
            "validation cost:  0.4675797876094895\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================================>.................................................................\n",
            "training iteration: 3500/10000\n",
            "validation cost:  0.4639123438801748\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================================>................................................................\n",
            "training iteration: 3600/10000\n",
            "validation cost:  0.4541225232010562\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================================>...............................................................\n",
            "training iteration: 3700/10000\n",
            "validation cost:  0.44016837598200603\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================================>..............................................................\n",
            "training iteration: 3800/10000\n",
            "validation cost:  0.43305752754857435\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================================>.............................................................\n",
            "training iteration: 3900/10000\n",
            "validation cost:  0.4299493981659793\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================>............................................................\n",
            "training iteration: 4000/10000\n",
            "validation cost:  0.4231567284725173\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================================>...........................................................\n",
            "training iteration: 4100/10000\n",
            "validation cost:  0.42293693687236955\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================================>..........................................................\n",
            "training iteration: 4200/10000\n",
            "validation cost:  0.4210213512329587\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================================>.........................................................\n",
            "training iteration: 4300/10000\n",
            "validation cost:  0.42238292218048323\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================================>........................................................\n",
            "training iteration: 4400/10000\n",
            "validation cost:  0.4057994719366793\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============================================>.......................................................\n",
            "training iteration: 4500/10000\n",
            "validation cost:  0.40516737569659944\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================================>......................................................\n",
            "training iteration: 4600/10000\n",
            "validation cost:  0.39275477868365566\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================================>.....................................................\n",
            "training iteration: 4700/10000\n",
            "validation cost:  0.39142635519727453\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================================>....................................................\n",
            "training iteration: 4800/10000\n",
            "validation cost:  0.38713945874623795\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================================>...................................................\n",
            "training iteration: 4900/10000\n",
            "validation cost:  0.3833566035585521\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================================================>..................................................\n",
            "training iteration: 5000/10000\n",
            "validation cost:  0.3809756590366034\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================================================>.................................................\n",
            "training iteration: 5100/10000\n",
            "validation cost:  0.38044668180944907\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================================================>................................................\n",
            "training iteration: 5200/10000\n",
            "validation cost:  0.3688359073073935\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================================================>...............................................\n",
            "training iteration: 5300/10000\n",
            "validation cost:  0.3688840447870437\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================================================>..............................................\n",
            "training iteration: 5400/10000\n",
            "validation cost:  0.36741311740153704\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================================================>.............................................\n",
            "training iteration: 5500/10000\n",
            "validation cost:  0.36396433411313256\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================================>............................................\n",
            "training iteration: 5600/10000\n",
            "validation cost:  0.36316105339117105\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================================>............................................\n",
            "training iteration: 5700/10000\n",
            "validation cost:  0.36168224014370076\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================================================>...........................................\n",
            "training iteration: 5800/10000\n",
            "validation cost:  0.367473025633236\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================================================>.........................................\n",
            "training iteration: 5900/10000\n",
            "validation cost:  0.35224056210107735\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================================================>........................................\n",
            "training iteration: 6000/10000\n",
            "validation cost:  0.36344439878211554\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Training done after 9 epochs and 6089/10000 iterations\n",
            "Final Train Accuracy: 91.515\n",
            "Final Validation Accuracy: 90.608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqDtf1tyP-3h",
        "outputId": "9e3358a2-8df9-492a-9e18-679e7ef8aebc"
      },
      "source": [
        "test_acc = predict(x_test.T, y_test.T, trained_params) * 100\n",
        "print(\"Final Test Accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 90.840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT6LEynkPwRw"
      },
      "source": [
        "# Section 5 - Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WTGCnAYPSkL"
      },
      "source": [
        "def L_layer_model_batchnorm(X, Y, layer_dims, learning_rate, num_iterations, batch_size):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (height*weight, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  layer_dims: ndarray of shape (num_of_layers)\n",
        "  batch_size: the number of examples in a single training batch\n",
        "  \"\"\"\n",
        "\n",
        "  def train_val_split(X, Y, train_size=0.8):\n",
        "    \"\"\"\n",
        "    The function split randomly the training data into train and validation set with a ratio given by train_size parameter\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: ndarray of shape (height*weight, num_of_examples)\n",
        "    Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "    train_size: float between 0-1, represent the training size\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x_train: The actual training set data, numpy array of shape (input_size, num_training_examples)\n",
        "    y_train: The actual training labels, numpy array of shape (num_of_classes, num_training_examples)\n",
        "    x_validation: The validation set data, numpy array of shape (input_size, num_validation_examples)\n",
        "    y_validation: The validation labels, numpy array of shape (num_of_classes, num_validation_examples)\n",
        "    \"\"\"\n",
        "    num_of_examples = X.shape[1]\n",
        "    indices = np.random.permutation(num_of_examples)\n",
        "    num_of_train_examples = int(num_of_examples*train_size)\n",
        "    # Taking indexes of train and validation\n",
        "    training_idx, validation_idx = indices[:num_of_train_examples], indices[num_of_train_examples:]\n",
        "    # Splitting the data:\n",
        "    x_train, y_train, x_validation, y_validation = X[:, training_idx], Y[:, training_idx], X[:, validation_idx], Y[:, validation_idx]\n",
        "    return x_train, y_train, x_validation, y_validation\n",
        "  \n",
        "  # Splitting the data:\n",
        "  x_train, y_train, x_validation, y_validation = train_val_split(X, Y)\n",
        "\n",
        "  print(\"training examples: \", x_train.shape[1])\n",
        "  print(\"validation examples: \", x_validation.shape[1])\n",
        "  \n",
        "  # initialize\n",
        "  parameters = initialize_parameters(layer_dims)\n",
        "  costs = []\n",
        "  \n",
        "  # report variables\n",
        "  training_iteration = 0\n",
        "  epochs = 0\n",
        "\n",
        "  # stopping criterion variables\n",
        "  ITERATION_TO_IMPROVE = 100\n",
        "  MAX_SMALL_IMPROVEMENT = 0.001\n",
        "  stop = False\n",
        "  last_cost = np.inf\n",
        "\n",
        "  # storing the best weights achieved so far\n",
        "  last_parameters = parameters\n",
        "\n",
        "  while not stop:\n",
        "    epochs += 1\n",
        "\n",
        "    # split the data to batches\n",
        "    for batch_data, batch_targets in list(zip(np.array_split(x_train, x_train.shape[1]/batch_size, axis=1), np.array_split(y_train, y_train.shape[1]/batch_size, axis=1))):\n",
        "      training_iteration+=1\n",
        "      \n",
        "      # Update params by batch values:\n",
        "      AL, caches = L_model_forward(batch_data, parameters, True)\n",
        "      grads = L_model_backward(AL, batch_targets, caches)\n",
        "      parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "      # computing validation set cost\n",
        "      validation_AL, validation_caches = L_model_forward(x_validation, parameters, True)\n",
        "      validation_set_cost = compute_cost(validation_AL, y_validation)\n",
        "\n",
        "      if training_iteration % 100 == 0:\n",
        "        # save cost\n",
        "        costs.append(validation_set_cost)\n",
        "        print_progress = int((training_iteration/num_iterations)*100)\n",
        "        print(\"=\"+\"=\"*print_progress+\">\"+\".\"*(100-print_progress))\n",
        "        print(f\"training iteration: {training_iteration}/{num_iterations}\")\n",
        "        print(\"validation cost: \", validation_set_cost)\n",
        "        print(\"-\"*102)\n",
        "        \n",
        "      if validation_set_cost + MAX_SMALL_IMPROVEMENT >= last_cost:\n",
        "        ITERATION_TO_IMPROVE -= 1\n",
        "      else:\n",
        "        # update last cost, save best weights so far, and reset the iteration to improve\n",
        "        last_cost = validation_set_cost\n",
        "        last_parameters = parameters\n",
        "        ITERATION_TO_IMPROVE = 100\n",
        "      \n",
        "      if ITERATION_TO_IMPROVE == 0:\n",
        "        # stop training if had `ITERATION_TO_IMPROVE` iteration without or with `MAX_SMAL_IMPROVEMENT` improvement\n",
        "        stop=True\n",
        "        break\n",
        "    \n",
        "      if training_iteration == num_iterations:\n",
        "        stop=True\n",
        "        break\n",
        "  \n",
        "  print(f\"Batch Normalization Training done after {epochs} epochs and {training_iteration}/{num_iterations} iterations\")\n",
        "  final_train_acc = predict_batchnorm(x_train, y_train, last_parameters)\n",
        "  final_validation_acc = predict_batchnorm(x_validation, y_validation, last_parameters)\n",
        "  print(\"Batch Normalization Final Train Accuracy: {:.3f}\".format(final_train_acc*100))\n",
        "  print(\"Batch Normalization Final Validation Accuracy: {:.3f}\".format(final_validation_acc*100))\n",
        "  return last_parameters, costs\n",
        "\n",
        "def predict_batchnorm(X, Y, parameters):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (height*weight, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  parameters: network learned weights\n",
        "  \"\"\"\n",
        "  # shape: (num_of_classes, num_of_examples)\n",
        "  outputs, caches = L_model_forward(X, parameters, True)\n",
        "  # shape: (num_of_classes, num_of_examples)\n",
        "  preds = np.zeros_like(outputs)\n",
        "  # shape: (num_of_examples)\n",
        "  network_argmax = np.argmax(outputs, axis=0)\n",
        "  preds[network_argmax, np.arange(preds.shape[1])] = 1\n",
        "\n",
        "  # calculate accuracy\n",
        "  correct_class = 0\n",
        "  num_of_examples = Y.shape[1]\n",
        "  for sample_index in range(num_of_examples):\n",
        "    sample_pred = preds[:, sample_index]\n",
        "    sample_target = Y[:, sample_index]\n",
        "    if np.array_equal(sample_pred, sample_target):\n",
        "      correct_class += 1\n",
        "  \n",
        "  accuracy = correct_class / num_of_examples\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yd4L2Xmprd8",
        "outputId": "1e0ca303-bbf7-488c-d842-23fccef12046"
      },
      "source": [
        "np.random.seed(42)\n",
        "print(\"Start Batch Normalization Training\")\n",
        "trained_params_batchnorm, costs_batchnorm = L_layer_model_batchnorm(x_train.T, y_train.T, layer_dims, 0.009, 10000, 64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples:  48000\n",
            "validation examples:  12000\n",
            "==>...................................................................................................\n",
            "training iteration: 100/10000\n",
            "validation cost:  2.1160924118522866\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===>..................................................................................................\n",
            "training iteration: 200/10000\n",
            "validation cost:  2.004428277426092\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====>.................................................................................................\n",
            "training iteration: 300/10000\n",
            "validation cost:  1.9010776328349481\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====>................................................................................................\n",
            "training iteration: 400/10000\n",
            "validation cost:  1.8075208401001306\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======>...............................................................................................\n",
            "training iteration: 500/10000\n",
            "validation cost:  1.743277083263888\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======>..............................................................................................\n",
            "training iteration: 600/10000\n",
            "validation cost:  1.6777651174213744\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========>.............................................................................................\n",
            "training iteration: 700/10000\n",
            "validation cost:  1.6295573319348942\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========>............................................................................................\n",
            "training iteration: 800/10000\n",
            "validation cost:  1.580726381143164\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========>...........................................................................................\n",
            "training iteration: 900/10000\n",
            "validation cost:  1.5426011789656344\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========>..........................................................................................\n",
            "training iteration: 1000/10000\n",
            "validation cost:  1.5028848751200803\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============>.........................................................................................\n",
            "training iteration: 1100/10000\n",
            "validation cost:  1.4632138334098255\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============>........................................................................................\n",
            "training iteration: 1200/10000\n",
            "validation cost:  1.429478936854182\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============>.......................................................................................\n",
            "training iteration: 1300/10000\n",
            "validation cost:  1.3980315786937723\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============>......................................................................................\n",
            "training iteration: 1400/10000\n",
            "validation cost:  1.3648787162114837\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================>.....................................................................................\n",
            "training iteration: 1500/10000\n",
            "validation cost:  1.3350599723096626\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================>....................................................................................\n",
            "training iteration: 1600/10000\n",
            "validation cost:  1.308788789830447\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================>...................................................................................\n",
            "training iteration: 1700/10000\n",
            "validation cost:  1.2846551556792338\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================>..................................................................................\n",
            "training iteration: 1800/10000\n",
            "validation cost:  1.2545641815799495\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================>.................................................................................\n",
            "training iteration: 1900/10000\n",
            "validation cost:  1.2359722095673282\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================>................................................................................\n",
            "training iteration: 2000/10000\n",
            "validation cost:  1.2114857133717125\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================>...............................................................................\n",
            "training iteration: 2100/10000\n",
            "validation cost:  1.186052978665697\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================>..............................................................................\n",
            "training iteration: 2200/10000\n",
            "validation cost:  1.166911123850316\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================>.............................................................................\n",
            "training iteration: 2300/10000\n",
            "validation cost:  1.1424571725051151\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================>............................................................................\n",
            "training iteration: 2400/10000\n",
            "validation cost:  1.12210836790196\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================>...........................................................................\n",
            "training iteration: 2500/10000\n",
            "validation cost:  1.1072132428662247\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================>..........................................................................\n",
            "training iteration: 2600/10000\n",
            "validation cost:  1.088291076856512\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================>.........................................................................\n",
            "training iteration: 2700/10000\n",
            "validation cost:  1.069527868434051\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================>........................................................................\n",
            "training iteration: 2800/10000\n",
            "validation cost:  1.054239809025953\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================>........................................................................\n",
            "training iteration: 2900/10000\n",
            "validation cost:  1.035062036205642\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================>......................................................................\n",
            "training iteration: 3000/10000\n",
            "validation cost:  1.020418414475076\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================>.....................................................................\n",
            "training iteration: 3100/10000\n",
            "validation cost:  1.0104074907434948\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================>....................................................................\n",
            "training iteration: 3200/10000\n",
            "validation cost:  0.9926226395256933\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================>...................................................................\n",
            "training iteration: 3300/10000\n",
            "validation cost:  0.9773131787628558\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================================>..................................................................\n",
            "training iteration: 3400/10000\n",
            "validation cost:  0.9634280337387536\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================================>.................................................................\n",
            "training iteration: 3500/10000\n",
            "validation cost:  0.9524541830612046\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================================>................................................................\n",
            "training iteration: 3600/10000\n",
            "validation cost:  0.9347581624440364\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================================>...............................................................\n",
            "training iteration: 3700/10000\n",
            "validation cost:  0.9203868783266407\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================================>..............................................................\n",
            "training iteration: 3800/10000\n",
            "validation cost:  0.9032164405288994\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================================>.............................................................\n",
            "training iteration: 3900/10000\n",
            "validation cost:  0.8892639119205267\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================>............................................................\n",
            "training iteration: 4000/10000\n",
            "validation cost:  0.8815616047736512\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================================>...........................................................\n",
            "training iteration: 4100/10000\n",
            "validation cost:  0.864844682501906\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================================>..........................................................\n",
            "training iteration: 4200/10000\n",
            "validation cost:  0.8556502418142584\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================================>.........................................................\n",
            "training iteration: 4300/10000\n",
            "validation cost:  0.8426571508455108\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================================>........................................................\n",
            "training iteration: 4400/10000\n",
            "validation cost:  0.8282023282124683\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============================================>.......................................................\n",
            "training iteration: 4500/10000\n",
            "validation cost:  0.8141031275759057\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================================>......................................................\n",
            "training iteration: 4600/10000\n",
            "validation cost:  0.807090423377981\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================================>.....................................................\n",
            "training iteration: 4700/10000\n",
            "validation cost:  0.7927602982545325\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================================>....................................................\n",
            "training iteration: 4800/10000\n",
            "validation cost:  0.7815739820501015\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================================>...................................................\n",
            "training iteration: 4900/10000\n",
            "validation cost:  0.7740015231167027\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================================================>..................................................\n",
            "training iteration: 5000/10000\n",
            "validation cost:  0.7651764270800814\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================================================>.................................................\n",
            "training iteration: 5100/10000\n",
            "validation cost:  0.7540985455903444\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================================================>................................................\n",
            "training iteration: 5200/10000\n",
            "validation cost:  0.7397389084912177\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================================================>...............................................\n",
            "training iteration: 5300/10000\n",
            "validation cost:  0.7272222067903997\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================================================>..............................................\n",
            "training iteration: 5400/10000\n",
            "validation cost:  0.7183621563675644\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================================================>.............................................\n",
            "training iteration: 5500/10000\n",
            "validation cost:  0.7130205742055096\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================================>............................................\n",
            "training iteration: 5600/10000\n",
            "validation cost:  0.7026666584121974\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================================>............................................\n",
            "training iteration: 5700/10000\n",
            "validation cost:  0.6971290008595633\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================================================>...........................................\n",
            "training iteration: 5800/10000\n",
            "validation cost:  0.6884392217623804\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================================================>.........................................\n",
            "training iteration: 5900/10000\n",
            "validation cost:  0.6805536229500847\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================================================>........................................\n",
            "training iteration: 6000/10000\n",
            "validation cost:  0.6686743553319023\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============================================================>.......................................\n",
            "training iteration: 6100/10000\n",
            "validation cost:  0.6664767905246993\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================================================>......................................\n",
            "training iteration: 6200/10000\n",
            "validation cost:  0.6543142933486545\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================================================>.....................................\n",
            "training iteration: 6300/10000\n",
            "validation cost:  0.6486388967796655\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================================================>....................................\n",
            "training iteration: 6400/10000\n",
            "validation cost:  0.6427906507676753\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================================================>...................................\n",
            "training iteration: 6500/10000\n",
            "validation cost:  0.6370186148023382\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================================================================>..................................\n",
            "training iteration: 6600/10000\n",
            "validation cost:  0.6296393422489955\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================================================================>.................................\n",
            "training iteration: 6700/10000\n",
            "validation cost:  0.621189455828019\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================================================================>................................\n",
            "training iteration: 6800/10000\n",
            "validation cost:  0.6135450880077922\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================================================================>...............................\n",
            "training iteration: 6900/10000\n",
            "validation cost:  0.6096353106012447\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================================================================>..............................\n",
            "training iteration: 7000/10000\n",
            "validation cost:  0.6069074758665982\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================================================================>.............................\n",
            "training iteration: 7100/10000\n",
            "validation cost:  0.5993170465336312\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================================================>............................\n",
            "training iteration: 7200/10000\n",
            "validation cost:  0.5984240633759452\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================================================================>...........................\n",
            "training iteration: 7300/10000\n",
            "validation cost:  0.5915518549719514\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================================================================>..........................\n",
            "training iteration: 7400/10000\n",
            "validation cost:  0.58480370044316\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================================================================>.........................\n",
            "training iteration: 7500/10000\n",
            "validation cost:  0.5783179220497424\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================================================================>........................\n",
            "training iteration: 7600/10000\n",
            "validation cost:  0.5791264469533467\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============================================================================>.......................\n",
            "training iteration: 7700/10000\n",
            "validation cost:  0.5714923024723987\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================================================================>......................\n",
            "training iteration: 7800/10000\n",
            "validation cost:  0.5690843510137744\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================================================================>.....................\n",
            "training iteration: 7900/10000\n",
            "validation cost:  0.5661827441259923\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================================================================>....................\n",
            "training iteration: 8000/10000\n",
            "validation cost:  0.5629964030080006\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================================================================>...................\n",
            "training iteration: 8100/10000\n",
            "validation cost:  0.5559174481681093\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================================================================================>..................\n",
            "training iteration: 8200/10000\n",
            "validation cost:  0.5503219380004295\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================================================================================>.................\n",
            "training iteration: 8300/10000\n",
            "validation cost:  0.5460416883146544\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================================================================================>................\n",
            "training iteration: 8400/10000\n",
            "validation cost:  0.5435904475540948\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================================================================================>...............\n",
            "training iteration: 8500/10000\n",
            "validation cost:  0.5446568923415065\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================================================================================>..............\n",
            "training iteration: 8600/10000\n",
            "validation cost:  0.537450547919946\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================================================================================>.............\n",
            "training iteration: 8700/10000\n",
            "validation cost:  0.5364752713205095\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================================================================================>............\n",
            "training iteration: 8800/10000\n",
            "validation cost:  0.5320531713214082\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================================================================================>...........\n",
            "training iteration: 8900/10000\n",
            "validation cost:  0.526455794416681\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================================================================================>..........\n",
            "training iteration: 9000/10000\n",
            "validation cost:  0.5210936976312577\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================================================================================>.........\n",
            "training iteration: 9100/10000\n",
            "validation cost:  0.5221193818831218\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================================================================================>........\n",
            "training iteration: 9200/10000\n",
            "validation cost:  0.5175715383500277\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============================================================================================>.......\n",
            "training iteration: 9300/10000\n",
            "validation cost:  0.5167183215777299\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================================================================================>......\n",
            "training iteration: 9400/10000\n",
            "validation cost:  0.5130815493132942\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Training done after 13 epochs and 9448/10000 iterations\n",
            "Final Train Accuracy: 87.379\n",
            "Final Validation Accuracy: 87.067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqUfeAHJy-7E",
        "outputId": "d2babdd2-9772-424b-f59a-ecf004433bbc"
      },
      "source": [
        "test_acc_batchnrom = predict_batchnorm(x_test.T, y_test.T, trained_params_batchnorm) * 100\n",
        "print(\"Batch Normalization Final Test Accuracy: {:.3f}\".format(test_acc_batchnrom))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 87.630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTrw5TtTUyA6"
      },
      "source": [
        "# Section 6 - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPh8lo5nWWee"
      },
      "source": [
        "def apply_dropout(A_new, keep_prob):\n",
        "  \"\"\"\n",
        "  A: ndarray of shape (size_of_current_layer, num_of_examples)\n",
        "  keep_prob: float number indecating the keep probability in the dropout layer\n",
        "  \"\"\"\n",
        "  # create the dropout matrix which indicate which neurons are turned off\n",
        "  dropout_vector = np.random.rand(A_new.shape[0], A_new.shape[1]) < keep_prob\n",
        "  # \"activate\" the dropout\n",
        "  A_dropout = np.multiply(A_new, dropout_vector)\n",
        "  # normalize the output from the dropout layer in order to reduce the effect of the turned off neurons\n",
        "  A_dropout /= keep_prob\n",
        "  dropout_cache = {\n",
        "      \"dropout_vector\": dropout_vector,\n",
        "      \"keep_prob\": keep_prob\n",
        "  }\n",
        "  return A_dropout, dropout_cache\n",
        "\n",
        "def L_model_forward_dropout(X, parameters, use_batchnorm, dropout_keep_prob):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (input_size, num_of_examples)\n",
        "  parameters: python's dictionary containing the initilized weights of the network\n",
        "  use_batchnorm: a boolean flag used to determine whether to apply batchnorm after activation - NOT SUPPORTED WITH DROPOUT\n",
        "  dropout_keep_prob: indicating the keep probability for every neuron in every layer\n",
        "  \"\"\"\n",
        "  num_of_layers = int(len(parameters.keys())/2)\n",
        "\n",
        "  A_prev = X\n",
        "  W = None\n",
        "  B = None\n",
        "  caches = []\n",
        "  # the forward-propagation until the output layer\n",
        "  for layer_index in range(1, num_of_layers):\n",
        "    W = parameters[f\"W{layer_index}\"]\n",
        "    B = parameters[f\"B{layer_index}\"]\n",
        "    A_new, cache = linear_activation_forward(A_prev, W, B, relu)\n",
        "\n",
        "    # call dropout\n",
        "    A_dropout, dropout_cache = apply_dropout(A_new, dropout_keep_prob)\n",
        "    cache.update(dropout_cache)\n",
        "\n",
        "    # add cache to caches and update A_prev\n",
        "    caches.append(cache)\n",
        "    A_prev = A_dropout\n",
        "  \n",
        "  # forward the output layer\n",
        "  W_output = parameters[f\"W{num_of_layers}\"]\n",
        "  B_output = parameters[f\"B{num_of_layers}\"]\n",
        "  AL, cache_output = linear_activation_forward(A_prev, W_output, B_output, softmax)\n",
        "  \n",
        "  caches.append(cache_output)\n",
        "  return AL, caches\n",
        "\n",
        "def dropout_backward(dA_dropout, dropout_cache):\n",
        "  \"\"\"\n",
        "  dA_dropout: ndarray of shape: (size_of_current_layer, num_of_examples)\n",
        "  dropout_cache: containing the dropout_vector and the keep_prob used in the forward propagation in this dropout layer\n",
        "  \"\"\"\n",
        "  dropout_vector, keep_prob = dropout_cache\n",
        "  dA = np.multiply(dA_dropout, dropout_vector*(1/keep_prob))\n",
        "  return dA\n",
        "\n",
        "def L_model_backward_dropout(AL, Y, caches):\n",
        "  \"\"\"\n",
        "  AL: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  caches: all of the caches from the forward propagation. In dropout implementation, it contains also the dropout cache (dropout_vector, A_new)\n",
        "  \"\"\"\n",
        "  grads = {}\n",
        "  num_of_layers = len(caches)\n",
        "\n",
        "  # calculating the gradients of the output layer\n",
        "  caches[-1].update({\"AL\": AL})\n",
        "  dA_dropout, dW, dB = linear_activation_backward(Y, caches[-1], \"softmax\")\n",
        "  grads[f\"dA_dropout{num_of_layers}\"] = dA_dropout\n",
        "  grads[f\"dW{num_of_layers}\"] = dW\n",
        "  grads[f\"dB{num_of_layers}\"] = dB\n",
        "\n",
        "  # calculating the gradients of all of the other hidden layers\n",
        "  for cache_index in reversed(range(num_of_layers-1)):\n",
        "    dropout_cache = (caches[cache_index]['dropout_vector'], caches[cache_index]['keep_prob'])\n",
        "    dA = dropout_backward(dA_dropout, dropout_cache)\n",
        "\n",
        "    dA_dropout, dW, dB = linear_activation_backward(dA, caches[cache_index], \"relu\")\n",
        "    grads[f\"dA_dropout{cache_index+1}\"] = dA_dropout\n",
        "    grads[f\"dW{cache_index+1}\"] = dW\n",
        "    grads[f\"dB{cache_index+1}\"] = dB\n",
        "  \n",
        "  return grads\n",
        "\n",
        "def L_layer_model_dropout(X, Y, layer_dims, learning_rate, num_iterations, batch_size, dropout_keep_prob=0.8):\n",
        "  \"\"\"\n",
        "  X: ndarray of shape (height*weight, num_of_examples)\n",
        "  Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "  layer_dims: ndarray of shape (num_of_layers)\n",
        "  batch_size: the number of examples in a single training batch\n",
        "  \"\"\"\n",
        "\n",
        "  def train_val_split(X, Y, train_size=0.8):\n",
        "    \"\"\"\n",
        "    The function split randomly the training data into train and validation set with a ratio given by train_size parameter\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: ndarray of shape (height*weight, num_of_examples)\n",
        "    Y: ndarray of shape (num_of_classes, num_of_examples)\n",
        "    train_size: float between 0-1, represent the training size\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x_train: The actual training set data, numpy array of shape (input_size, num_training_examples)\n",
        "    y_train: The actual training labels, numpy array of shape (num_of_classes, num_training_examples)\n",
        "    x_validation: The validation set data, numpy array of shape (input_size, num_validation_examples)\n",
        "    y_validation: The validation labels, numpy array of shape (num_of_classes, num_validation_examples)\n",
        "    \"\"\"\n",
        "    num_of_examples = X.shape[1]\n",
        "    indices = np.random.permutation(num_of_examples)\n",
        "    num_of_train_examples = int(num_of_examples*train_size)\n",
        "    # Taking indexes of train and validation\n",
        "    training_idx, validation_idx = indices[:num_of_train_examples], indices[num_of_train_examples:]\n",
        "    # Splitting the data:\n",
        "    x_train, y_train, x_validation, y_validation = X[:, training_idx], Y[:, training_idx], X[:, validation_idx], Y[:, validation_idx]\n",
        "    return x_train, y_train, x_validation, y_validation\n",
        "  \n",
        "  # Splitting the data:\n",
        "  x_train, y_train, x_validation, y_validation = train_val_split(X, Y)\n",
        "\n",
        "  print(\"training examples: \", x_train.shape[1])\n",
        "  print(\"validation examples: \", x_validation.shape[1])\n",
        "  \n",
        "  # initialize\n",
        "  parameters = initialize_parameters(layer_dims)\n",
        "  costs = []\n",
        "  \n",
        "  # report variables\n",
        "  training_iteration = 0\n",
        "  epochs = 0\n",
        "\n",
        "  # stopping criterion variables\n",
        "  ITERATION_TO_IMPROVE = 100\n",
        "  MAX_SMALL_IMPROVEMENT = 0.001\n",
        "  stop = False\n",
        "  last_cost = np.inf\n",
        "\n",
        "  # storing the best weights achieved so far\n",
        "  last_parameters = parameters\n",
        "\n",
        "  while not stop:\n",
        "    epochs += 1\n",
        "\n",
        "    # split the data to batches\n",
        "    for batch_data, batch_targets in list(zip(np.array_split(x_train, x_train.shape[1]/batch_size, axis=1), np.array_split(y_train, y_train.shape[1]/batch_size, axis=1))):\n",
        "      training_iteration+=1\n",
        "      \n",
        "      # Update params by batch values:\n",
        "      AL, caches = L_model_forward_dropout(batch_data, parameters, False, dropout_keep_prob)\n",
        "      grads = L_model_backward_dropout(AL, batch_targets, caches)\n",
        "      parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "      # computing validation set cost\n",
        "      validation_AL, validation_caches = L_model_forward_dropout(x_validation, parameters, False, dropout_keep_prob)\n",
        "      validation_set_cost = compute_cost(validation_AL, y_validation)\n",
        "\n",
        "      if training_iteration % 100 == 0:\n",
        "        # save cost\n",
        "        costs.append(validation_set_cost)\n",
        "        print_progress = int((training_iteration/num_iterations)*100)\n",
        "        print(\"=\"+\"=\"*print_progress+\">\"+\".\"*(100-print_progress))\n",
        "        print(f\"training iteration: {training_iteration}/{num_iterations}\")\n",
        "        print(\"validation cost: \", validation_set_cost)\n",
        "        print(\"-\"*102)\n",
        "        \n",
        "      if validation_set_cost + MAX_SMALL_IMPROVEMENT >= last_cost:\n",
        "        ITERATION_TO_IMPROVE -= 1\n",
        "      else:\n",
        "        # update last cost, save best weights so far, and reset the iteration to improve\n",
        "        last_cost = validation_set_cost\n",
        "        last_parameters = parameters\n",
        "        ITERATION_TO_IMPROVE = 100\n",
        "      \n",
        "      if ITERATION_TO_IMPROVE == 0:\n",
        "        # stop training if had `ITERATION_TO_IMPROVE` iteration without or with `MAX_SMAL_IMPROVEMENT` improvement\n",
        "        stop=True\n",
        "        break\n",
        "    \n",
        "      if training_iteration == num_iterations:\n",
        "        stop=True\n",
        "        break\n",
        "\n",
        "\n",
        "  print(f\"Dropout Training done after {epochs} epochs and {training_iteration}/{num_iterations} iterations\")\n",
        "  final_train_acc = predict(x_train, y_train, last_parameters)\n",
        "  final_validation_acc = predict(x_validation, y_validation, last_parameters)\n",
        "  print(\"Dropout Final Train Accuracy: {:.3f}\".format(final_train_acc*100))\n",
        "  print(\"Dropout Final Validation Accuracy: {:.3f}\".format(final_validation_acc*100))\n",
        "  return last_parameters, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdPZN7q5mRy7",
        "outputId": "9caf7647-42b5-4a3c-b72d-eaae8bc4850a"
      },
      "source": [
        "np.random.seed(42)\n",
        "print(\"Start Dropout Training\")\n",
        "trained_params_dropout, costs_dropout = L_layer_model_dropout(x_train.T, y_train.T, layer_dims, 0.009, 10000, 64, 0.8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples:  48000\n",
            "validation examples:  12000\n",
            "==>...................................................................................................\n",
            "training iteration: 100/10000\n",
            "validation cost:  2.2266398088147015\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===>..................................................................................................\n",
            "training iteration: 200/10000\n",
            "validation cost:  2.1516442111213756\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====>.................................................................................................\n",
            "training iteration: 300/10000\n",
            "validation cost:  2.0970634081000257\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====>................................................................................................\n",
            "training iteration: 400/10000\n",
            "validation cost:  2.0442897680957848\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======>...............................................................................................\n",
            "training iteration: 500/10000\n",
            "validation cost:  1.999870761369679\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======>..............................................................................................\n",
            "training iteration: 600/10000\n",
            "validation cost:  1.9650277516443693\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========>.............................................................................................\n",
            "training iteration: 700/10000\n",
            "validation cost:  1.9297428788184503\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========>............................................................................................\n",
            "training iteration: 800/10000\n",
            "validation cost:  1.900595166586044\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========>...........................................................................................\n",
            "training iteration: 900/10000\n",
            "validation cost:  1.869814747012941\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========>..........................................................................................\n",
            "training iteration: 1000/10000\n",
            "validation cost:  1.836772373446936\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============>.........................................................................................\n",
            "training iteration: 1100/10000\n",
            "validation cost:  1.829323609063044\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============>........................................................................................\n",
            "training iteration: 1200/10000\n",
            "validation cost:  1.8053079154850882\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==============>.......................................................................................\n",
            "training iteration: 1300/10000\n",
            "validation cost:  1.7747176799912492\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============>......................................................................................\n",
            "training iteration: 1400/10000\n",
            "validation cost:  1.7518768639679845\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================>.....................................................................................\n",
            "training iteration: 1500/10000\n",
            "validation cost:  1.7524033190874\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================>....................................................................................\n",
            "training iteration: 1600/10000\n",
            "validation cost:  1.7345125306581066\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================>...................................................................................\n",
            "training iteration: 1700/10000\n",
            "validation cost:  1.7201370867264771\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===================>..................................................................................\n",
            "training iteration: 1800/10000\n",
            "validation cost:  1.700953587203515\n",
            "------------------------------------------------------------------------------------------------------\n",
            "====================>.................................................................................\n",
            "training iteration: 1900/10000\n",
            "validation cost:  1.6705539479763711\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=====================>................................................................................\n",
            "training iteration: 2000/10000\n",
            "validation cost:  1.6739145870675514\n",
            "------------------------------------------------------------------------------------------------------\n",
            "======================>...............................................................................\n",
            "training iteration: 2100/10000\n",
            "validation cost:  1.6549818396518048\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=======================>..............................................................................\n",
            "training iteration: 2200/10000\n",
            "validation cost:  1.6311636840717982\n",
            "------------------------------------------------------------------------------------------------------\n",
            "========================>.............................................................................\n",
            "training iteration: 2300/10000\n",
            "validation cost:  1.6273999362197105\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=========================>............................................................................\n",
            "training iteration: 2400/10000\n",
            "validation cost:  1.6179120949469161\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==========================>...........................................................................\n",
            "training iteration: 2500/10000\n",
            "validation cost:  1.6029034870724688\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===========================>..........................................................................\n",
            "training iteration: 2600/10000\n",
            "validation cost:  1.5736629039167838\n",
            "------------------------------------------------------------------------------------------------------\n",
            "============================>.........................................................................\n",
            "training iteration: 2700/10000\n",
            "validation cost:  1.5693378346183808\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================>........................................................................\n",
            "training iteration: 2800/10000\n",
            "validation cost:  1.5690474777902248\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=============================>........................................................................\n",
            "training iteration: 2900/10000\n",
            "validation cost:  1.5425209082205495\n",
            "------------------------------------------------------------------------------------------------------\n",
            "===============================>......................................................................\n",
            "training iteration: 3000/10000\n",
            "validation cost:  1.5484159328387699\n",
            "------------------------------------------------------------------------------------------------------\n",
            "================================>.....................................................................\n",
            "training iteration: 3100/10000\n",
            "validation cost:  1.5370584852766151\n",
            "------------------------------------------------------------------------------------------------------\n",
            "=================================>....................................................................\n",
            "training iteration: 3200/10000\n",
            "validation cost:  1.5225302871859137\n",
            "------------------------------------------------------------------------------------------------------\n",
            "==================================>...................................................................\n",
            "training iteration: 3300/10000\n",
            "validation cost:  1.5147792604223893\n",
            "------------------------------------------------------------------------------------------------------\n",
            "Training done after 5 epochs and 3334/10000 iterations\n",
            "Final Train Accuracy: 68.233\n",
            "Final Validation Accuracy: 68.275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkyEEPs74Fm1",
        "outputId": "e2a929c7-5a65-49d1-c494-8d8dfbccc8b6"
      },
      "source": [
        "test_acc_dropout = predict(x_test.T, y_test.T, trained_params_dropout) * 100\n",
        "print(\"Dropout Final Test Accuracy: {:.3f}\".format(test_acc_dropout))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 67.650\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}